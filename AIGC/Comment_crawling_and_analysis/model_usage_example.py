# æ¨¡å‹è°ƒç”¨ç¤ºä¾‹ä»£ç 
import os
import jieba
import re
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Layer
from gensim.models import Word2Vec
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.saving import register_keras_serializable

# è‡ªå®šä¹‰å±‚å®šä¹‰ï¼ˆç‹¬ç«‹å·¥ä½œç‰ˆæœ¬ï¼‰
@register_keras_serializable(package='Custom')
class BertIntegrationLayer(Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._dtype = tf.int32
        self.bert_model = None
    
    def build(self, input_shape):
        # å»¶è¿ŸåŠ è½½BERTæ¨¡å‹
        if self.bert_model is None:
            self.bert_model = TFBertModel.from_pretrained('bert-base-chinese', from_pt=True)
        self._trainable_weights = []
    
    def call(self, inputs):
        input_ids = tf.cast(inputs[0], self._dtype)
        attention_mask = tf.cast(inputs[1], self._dtype)
        outputs = self.bert_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        return outputs.last_hidden_state
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0][0], input_shape[0][1], 768)

# 1. åŠ è½½æ¨¡å‹
# å…ˆåŠ è½½BERTæ¨¡å‹ç”¨äºè‡ªå®šä¹‰å±‚
try:
    bert_model = TFBertModel.from_pretrained('bert-base-chinese', from_pt=True)
except Exception as e:
    print(f"BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# ä½¿ç”¨custom_objectsåŠ è½½åˆ†ç±»æ¨¡å‹
model = load_model(
    r'AIGC\Comment_crawling_and_analysis\review_classifier.keras',
    custom_objects={'BertIntegrationLayer': BertIntegrationLayer}
)
w2v_model = Word2Vec.load(r'AIGC\Comment_crawling_and_analysis\word2vec_model.bin')
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

# 2. é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    # æ–‡æœ¬æ¸…æ´—
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # åˆ†è¯
    tokens = [w for w in jieba.lcut(text) if w.strip()]
    
    # æ„å»ºWord2Vecè¯æ±‡è¡¨
    vocab = w2v_model.wv.index_to_key
    word_index = {word: idx+1 for idx, word in enumerate(vocab)}
    
    # è½¬æ¢ä¸ºWord2Vecåºåˆ—
    w2v_seq = [word_index.get(word, 0) for word in tokens]
    w2v_padded = pad_sequences([w2v_seq], maxlen=128, padding='post', dtype='int32')
    
    # å‡†å¤‡BERTè¾“å…¥
    bert_input = bert_tokenizer.encode_plus(
        text,
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='np'
    )
    
    return w2v_padded, bert_input['input_ids'], bert_input['attention_mask']

# åˆ†å¥å‡½æ•°
def split_sentences(text):
    """æ”¹è¿›çš„ä¸­æ–‡åˆ†å¥å‡½æ•°"""
    # å…ˆæŒ‰å¸¸è§æ ‡ç‚¹åˆ†å¥
    sentences = re.split(r'([ï¼Œ,ã€‚ï¼ï¼Ÿï¼›\.\!\?;])', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # åˆå¹¶æ ‡ç‚¹ç¬¦å·å›å‰ä¸€å¥
    merged = []
    for i in range(0, len(sentences)-1, 2):
        merged.append(sentences[i] + sentences[i+1])
    if len(sentences) % 2 == 1:
        merged.append(sentences[-1])
    
    # å¯¹é•¿å¥è¿›ä¸€æ­¥æ‹†åˆ†
    final_sentences = []
    for sent in merged:
        # å¦‚æœæœ‰è½¬æŠ˜è¯åˆ™æ‹†åˆ†
        if any(word in sent for word in ['ä½†','ä½†æ˜¯','ç„¶è€Œ','ä¸è¿‡','å´']):
            parts = re.split(r'(ä½†|ä½†æ˜¯|ç„¶è€Œ|ä¸è¿‡|å´)', sent)
            parts = [p.strip() for p in parts if p.strip()]
            for i in range(0, len(parts)-1, 2):
                final_sentences.append(parts[i] + parts[i+1])
            if len(parts) % 2 == 1:
                final_sentences.append(parts[-1])
        else:
            final_sentences.append(sent)
    
    return final_sentences

# 3. ç¤ºä¾‹é¢„æµ‹
def predict_sentiment(text, sentence_level=True):
    if sentence_level:
        # åˆ†å¥å¤„ç†
        sentences = split_sentences(text)
        if not sentences:
            return {'sentiment': 'ä¸­æ€§', 'confidence': 0, 'raw_score': 0.5}
        
        # åˆ†ææ¯ä¸ªåˆ†å¥
        sentence_scores = []
        for sent in sentences:
            w2v_input, bert_ids, bert_mask = preprocess_text(sent)
            score = model.predict([w2v_input, bert_ids, bert_mask])[0][0]
            sentence_scores.append(float(score))
        
        # å¢åŠ æ•´ä½“å¥å­åˆ†æ
        w2v_input_full, bert_ids_full, bert_mask_full = preprocess_text(text)
        full_text_score = model.predict([w2v_input_full, bert_ids_full, bert_mask_full])[0][0]
        
        # ä¼˜åŒ–åçš„æƒ…æ„Ÿè®¡ç®—ç®—æ³•
        # 1. åŠ¨æ€æƒé‡è®¡ç®—ï¼ˆæ›´å¹³ç¼“çš„æ›²çº¿ï¼‰
        weights = []
        for score in sentence_scores:
            # è°ƒæ•´åçš„Så‹æ›²çº¿ï¼Œæ–œç‡æ›´å¹³ç¼“
            weight = 1 + 0.6 / (1 + np.exp(8*(score-0.45)))
            weights.append(weight)
        
        # 2. ä½ç½®æƒé‡ï¼ˆåé¢çš„å¥å­ç¨é«˜ï¼‰
        weights = [w * (1 + 0.02*i) for i, w in enumerate(weights)]
        
        # 3. è®¡ç®—åŠ æƒåˆ†æ•°ï¼ˆä¿ç•™å¹³æ»‘å¤„ç†ï¼‰
        weighted_scores = [0.15 + 0.7 * score for score in sentence_scores]
        sentence_score = np.average(weighted_scores, weights=weights)
        
        # 4. ç»“åˆæ•´ä½“åˆ†æï¼ˆ0.7åˆ†å¥ + 0.3æ•´ä½“ï¼‰
        raw_score = sentence_score * 0.7 + full_text_score * 0.3
        
        # 4. è´Ÿé¢å†…å®¹æ£€æµ‹ï¼ˆè°ƒæ•´åçš„æ¡ä»¶ï¼‰
        strong_negative = any(s < 0.2 for s in sentence_scores)
        moderate_negative = any(s < 0.3 for s in sentence_scores)
        
        if strong_negative:
            # å¼ºçƒˆè´Ÿé¢å†…å®¹ï¼šé€‚åº¦é™ä½åˆ†æ•°
            raw_score = raw_score * 0.8
        elif moderate_negative:
            # ä¸€èˆ¬è´Ÿé¢å†…å®¹ï¼šè½»å¾®è°ƒæ•´
            raw_score = raw_score * 0.95
    else:
        # æ•´ä½“å¤„ç†ï¼ˆåŸé€»è¾‘ï¼‰
        w2v_input, bert_ids, bert_mask = preprocess_text(text)
        raw_score = model.predict([w2v_input, bert_ids, bert_mask])[0][0]
    
    # ä¼˜åŒ–åçš„æœ€ç»ˆåˆ¤æ–­é€»è¾‘
    if raw_score > 0.6:
        sentiment = 'æ­£é¢'
        confidence = raw_score
    elif raw_score < 0.35:
        if any(s < 0.25 for s in (sentence_scores if sentence_level else [raw_score])):
            sentiment = 'è´Ÿé¢'
            confidence = 1 - raw_score
        else:
            sentiment = 'ä¸­æ€§'
            confidence = 1 - abs(raw_score - 0.5)
    else:
        sentiment = 'ä¸­æ€§'
        confidence = 1 - abs(raw_score - 0.5)

    
    result = {
        'sentiment': sentiment,
        'confidence': float(confidence),
        'raw_score': float(raw_score),
        'sentence_scores': sentence_scores if sentence_level else None,
        'sentences': split_sentences(text) if sentence_level else None
    }
    
    if sentence_level:
        result['full_text_score'] = float(full_text_score)
    
    return result

# 4. ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    test_texts = [
        "ç»™ä¸ªå¥½è¯„éª—ä¸€ä¸‹æ›´å¤šçš„äººğŸ˜ï¼Œç‰¹åˆ«ç¾å‘³ï¼Œè‰²é¦™å‘³ä¿±å…¨ï¼Œå€¼äº†ã€‚çœŸæ£’ğŸ‘ğŸ»éå¸¸æ»¡æ„ã€‚,",
        "é™æ—¶ç‰¹ä»·ä¹°çš„ï¼Œé™æ—¶ä¸€ç»“æŸå°±é™20å…ƒï¼Œè¥é”€æœ‰ç‚¹é—®é¢˜ï¼Œä¸è¿‡è´¨é‡è¿˜å¯ä»¥ï¼Œç»™æ‰‹æœºå……ç”µå¾ˆå¿«",
        "åˆ°è´§å¾ˆå¿«ï¼Œå°±æ˜¯ç®±å­çƒ‚äº†ï¼Œä¸è¿‡æ²¡å°‘ä¸œè¥¿ã€‚å®¢æœå›å¤å¾ˆå¿«"
    ]
    
    for test_text in test_texts:
        result = predict_sentiment(test_text)
        print(f"\n=== åˆ†æç»“æœ ===")
        print(f"è¾“å…¥æ–‡æœ¬: {test_text}")
        print("\nåˆ†å¥åˆ†æ:")
        for i, (sent, score) in enumerate(zip(result['sentences'], result['sentence_scores'])):
            print(f" {i+1}. {sent} (åˆ†å¥å¾—åˆ†: {score:.4f})")
        if 'full_text_score' in result:
            print(f"\næ•´ä½“åˆ†æå¾—åˆ†: {result['full_text_score']:.4f}")
        print(f"ç»¼åˆå¾—åˆ†: {result['raw_score']:.4f} ({result['sentiment']})")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.2%}")
        
        # æ£€æµ‹å¹¶å¤„ç†æƒ…æ„Ÿå…³é”®è¯
        positive_words = ['å¥½','ä¼˜ç§€','æ£’','æ»¡æ„','èµ','æ¨è','å–œæ¬¢','è¶…èµ','æ€åº¦å¥½','é€Ÿåº¦å¿«','è´¨é‡å¥½']
        sarcasm_words = ['éª—','å‡è£…','é˜´é˜³','è®½åˆº','åè¯','å‡çš„','å¿«è·‘','åˆ«ä¿¡']
        neutral_phrases = ['ä¸ç¡®å®š','ä¸çŸ¥é“']
        negative_words = ['å·®','ä¸å¥½','ç³Ÿç³•','ä¸è¡Œ','ä¸æ¨è','çƒ‚','åƒåœ¾','å','åçš„','ä¸æ»¡æ„','å·®åŠ²','ä¸æ»¡','ä¸å¥½ç”¨','ä¸èˆ’æœ','ä¸çˆ½','éš¾ç”¨','éš¾å—','éš¾çœ‹']
        
        # æ£€æµ‹å…³é”®è¯
        has_positive = any(word in test_text for word in positive_words)
        has_sarcasm = any(word in test_text for word in sarcasm_words)
        has_negative = any(word in test_text for word in negative_words) and \
                      not any(phrase in test_text for phrase in neutral_phrases)
        
        # ä¼˜å…ˆå¤„ç†æ­£é¢è¯„ä»·
        if has_positive and not has_negative:
            print("â€» æ³¨æ„: æ£€æµ‹åˆ°æ­£é¢è¯„ä»·å†…å®¹ â€»")
            result['raw_score'] = min(1.0, result['raw_score'] * 1.2)  # å¢å¼ºæ­£é¢åˆ†æ•°
            result['sentiment'] = 'æ­£é¢'
            result['confidence'] = result['raw_score']
        elif has_sarcasm:
            print("â€» è­¦å‘Š: æ£€æµ‹åˆ°å¯èƒ½åŒ…å«è®½åˆºå†…å®¹ â€»")
            result['raw_score'] = max(0, min(1, 1 - result['raw_score']))
            result['sentiment'] = 'è´Ÿé¢' if result['raw_score'] <= 0.5 else 'æ­£é¢'
            result['confidence'] = result['raw_score'] if result['sentiment'] == 'æ­£é¢' else 1 - result['raw_score']
        elif has_negative:
            print("â€» æ³¨æ„: æ£€æµ‹åˆ°è´Ÿé¢è¯„ä»·å†…å®¹ â€»")
            result['raw_score'] = max(0, result['raw_score'] * 0.8)  # é™ä½è´Ÿé¢æƒ©ç½šåŠ›åº¦
            result['sentiment'] = 'è´Ÿé¢' if result['raw_score'] <= 0.5 else 'æ­£é¢'
            result['confidence'] = result['raw_score'] if result['sentiment'] == 'æ­£é¢' else 1 - result['raw_score']
